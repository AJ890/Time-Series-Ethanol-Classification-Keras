{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier model for the Multivariate Time Series data using LSTM in Keras\n",
    "\n",
    "\n",
    "This is a three-class classification problem for the given multivariate time series data. The \"goal\" column has three values 0 (no event), 1 (event of type 1), and 2 (event of type 2). Although we can treat events 1 and 2 as one \"positive\" class and 0 as the \"negative\" class, we will build a 3-class classifier and create a confusion matrix.  \n",
    "\n",
    "We use LSTM, which is a variation of RNN, for building our network.\n",
    "\n",
    "#### About the dataset:\n",
    "There are 17 columns in total, of which the first two columns are the IDs for each timeseries (to mark the beginning and end of each timeseries), and are therefore discarded. The next 14 columns are the variables listed in time in equal intervals, and the last column is the \"goal\" column. There are 89 samples in each time series. For example, rows 2 to 90 represent one time series, rows 91 to 178 represent the next time series, and so on.\n",
    "\n",
    "\n",
    "#### Lets start by importing the necessary modules and loading the raw dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bx    by    bz    bl    bm    bn  bmag      vx      vy     vz    vmag  \\\n",
      "0  0.52  3.99 -2.53  0.12 -4.74 -0.33  4.75 -212.49  126.52  94.29  264.67   \n",
      "1 -0.46  2.83 -0.37  1.47 -2.49 -0.08  2.89 -205.30  121.80  91.70  255.72   \n",
      "2  0.63  3.69 -2.23  0.10 -4.36 -0.12  4.36 -208.85  119.43  89.61  256.73   \n",
      "\n",
      "     np    tpar    tper  goal  \n",
      "0  5.51  141.67  157.74     2  \n",
      "1  6.51  139.22  154.13     2  \n",
      "2  6.15  144.39  151.92     2  \n",
      "\n",
      "values.shape = (18245, 15)\n",
      "scaled.shape = (18245, 15)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pandas import read_excel, DataFrame, concat\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Loading the raw dataset (ignore the first 2 columns A & B)\n",
    "dataset = read_excel(\"challenge_dataset.xlsx\", usecols=\"C:Q\")\n",
    "\n",
    "values = dataset.values\n",
    "\n",
    "print(dataset.head(3)) # Lets print the first 3 rows of the dataset to take a peek at what we have\n",
    "\n",
    "Nc = values.shape[1] # number of columns\n",
    "\n",
    "values = values.astype('float32') # ensuring all the data is float\n",
    "\n",
    "# normalizing features (columns 1 to 14) NOTE: column 15 = goal (three classes: 0, 1, 2)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values[:,0:Nc])\n",
    "\n",
    "print()\n",
    "print(\"values.shape =\", values.shape)\n",
    "print(\"scaled.shape =\", scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18245, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we normalized all the features so that they range between 0 and 1.\n",
    "\n",
    "Now we need to frame the raw dataset into a supervised learning problem.\n",
    "We set timestep equal to 1 (t_prev = 1), which means that we only look at the data in the previous row to train and predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reframed.shape before drop = (18244, 30)\n",
      "reframed.shape after  drop = (18244, 16)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# framing the problem as supervised learning\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame the time series as a supervised learning dataset.\n",
    "    \n",
    "    Arguments:\n",
    "        data:    Sequence of observations as a list or NumPy array.\n",
    "        n_in:    Number of lag observations as input (X).\n",
    "        n_out:   Number of observations as output (y).\n",
    "        dropnan: Boolean, whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # future sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n",
    "\n",
    "t_prev = 1 # timestep\n",
    "t_next = 1\n",
    "reframed = series_to_supervised(scaled, t_prev, t_next)\n",
    "print(\"reframed.shape before drop =\", reframed.shape)\n",
    "# dropping the columns that we don't want to predict\n",
    "reframed.drop(reframed.columns[[15,16,17,18,19,20,21,22,23,24,25,26,27,28]], axis=1, inplace=True)\n",
    "print(\"reframed.shape after  drop =\", reframed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "      <th>var6(t-1)</th>\n",
       "      <th>var7(t-1)</th>\n",
       "      <th>var8(t-1)</th>\n",
       "      <th>var9(t-1)</th>\n",
       "      <th>var10(t-1)</th>\n",
       "      <th>var11(t-1)</th>\n",
       "      <th>var12(t-1)</th>\n",
       "      <th>var13(t-1)</th>\n",
       "      <th>var14(t-1)</th>\n",
       "      <th>var15(t-1)</th>\n",
       "      <th>var15(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.830850</td>\n",
       "      <td>0.446224</td>\n",
       "      <td>0.631946</td>\n",
       "      <td>0.791192</td>\n",
       "      <td>0.511947</td>\n",
       "      <td>0.421807</td>\n",
       "      <td>0.017356</td>\n",
       "      <td>0.431023</td>\n",
       "      <td>0.764555</td>\n",
       "      <td>0.645218</td>\n",
       "      <td>0.362104</td>\n",
       "      <td>0.038302</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.020626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.826580</td>\n",
       "      <td>0.436446</td>\n",
       "      <td>0.646286</td>\n",
       "      <td>0.796235</td>\n",
       "      <td>0.531711</td>\n",
       "      <td>0.424278</td>\n",
       "      <td>0.008560</td>\n",
       "      <td>0.438072</td>\n",
       "      <td>0.759976</td>\n",
       "      <td>0.642756</td>\n",
       "      <td>0.349659</td>\n",
       "      <td>0.045255</td>\n",
       "      <td>0.013365</td>\n",
       "      <td>0.020129</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.831329</td>\n",
       "      <td>0.443695</td>\n",
       "      <td>0.633937</td>\n",
       "      <td>0.791117</td>\n",
       "      <td>0.515285</td>\n",
       "      <td>0.423883</td>\n",
       "      <td>0.015512</td>\n",
       "      <td>0.434592</td>\n",
       "      <td>0.757677</td>\n",
       "      <td>0.640769</td>\n",
       "      <td>0.351063</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.013923</td>\n",
       "      <td>0.019825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.836035</td>\n",
       "      <td>0.444538</td>\n",
       "      <td>0.636195</td>\n",
       "      <td>0.789810</td>\n",
       "      <td>0.514406</td>\n",
       "      <td>0.434361</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.437709</td>\n",
       "      <td>0.762431</td>\n",
       "      <td>0.642994</td>\n",
       "      <td>0.351884</td>\n",
       "      <td>0.042126</td>\n",
       "      <td>0.014391</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.831590</td>\n",
       "      <td>0.419504</td>\n",
       "      <td>0.649007</td>\n",
       "      <td>0.790930</td>\n",
       "      <td>0.546557</td>\n",
       "      <td>0.432187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431161</td>\n",
       "      <td>0.758783</td>\n",
       "      <td>0.646140</td>\n",
       "      <td>0.358530</td>\n",
       "      <td>0.041501</td>\n",
       "      <td>0.018190</td>\n",
       "      <td>0.020026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.841177</td>\n",
       "      <td>0.433075</td>\n",
       "      <td>0.639979</td>\n",
       "      <td>0.786037</td>\n",
       "      <td>0.525035</td>\n",
       "      <td>0.444543</td>\n",
       "      <td>0.013809</td>\n",
       "      <td>0.415143</td>\n",
       "      <td>0.777983</td>\n",
       "      <td>0.647804</td>\n",
       "      <td>0.390819</td>\n",
       "      <td>0.038302</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>0.020348</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.835338</td>\n",
       "      <td>0.480529</td>\n",
       "      <td>0.612162</td>\n",
       "      <td>0.789660</td>\n",
       "      <td>0.466883</td>\n",
       "      <td>0.420127</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.402939</td>\n",
       "      <td>0.798085</td>\n",
       "      <td>0.643593</td>\n",
       "      <td>0.417379</td>\n",
       "      <td>0.035521</td>\n",
       "      <td>0.019466</td>\n",
       "      <td>0.021215</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.832288</td>\n",
       "      <td>0.481035</td>\n",
       "      <td>0.614353</td>\n",
       "      <td>0.792051</td>\n",
       "      <td>0.469079</td>\n",
       "      <td>0.416469</td>\n",
       "      <td>0.040624</td>\n",
       "      <td>0.391617</td>\n",
       "      <td>0.786278</td>\n",
       "      <td>0.645674</td>\n",
       "      <td>0.422677</td>\n",
       "      <td>0.033366</td>\n",
       "      <td>0.019659</td>\n",
       "      <td>0.021469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.827930</td>\n",
       "      <td>0.465273</td>\n",
       "      <td>0.611830</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.482783</td>\n",
       "      <td>0.403124</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>0.390548</td>\n",
       "      <td>0.786123</td>\n",
       "      <td>0.649163</td>\n",
       "      <td>0.425417</td>\n",
       "      <td>0.034618</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.839129</td>\n",
       "      <td>0.482468</td>\n",
       "      <td>0.599549</td>\n",
       "      <td>0.783721</td>\n",
       "      <td>0.455288</td>\n",
       "      <td>0.416469</td>\n",
       "      <td>0.048759</td>\n",
       "      <td>0.388490</td>\n",
       "      <td>0.777546</td>\n",
       "      <td>0.643593</td>\n",
       "      <td>0.419549</td>\n",
       "      <td>0.031836</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.024456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.851155</td>\n",
       "      <td>0.450270</td>\n",
       "      <td>0.638452</td>\n",
       "      <td>0.784506</td>\n",
       "      <td>0.505358</td>\n",
       "      <td>0.464907</td>\n",
       "      <td>0.028092</td>\n",
       "      <td>0.388568</td>\n",
       "      <td>0.763954</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.417282</td>\n",
       "      <td>0.034896</td>\n",
       "      <td>0.019955</td>\n",
       "      <td>0.022581</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.828715</td>\n",
       "      <td>0.450691</td>\n",
       "      <td>0.634800</td>\n",
       "      <td>0.794292</td>\n",
       "      <td>0.510717</td>\n",
       "      <td>0.421313</td>\n",
       "      <td>0.018491</td>\n",
       "      <td>0.382549</td>\n",
       "      <td>0.742028</td>\n",
       "      <td>0.684259</td>\n",
       "      <td>0.428504</td>\n",
       "      <td>0.035521</td>\n",
       "      <td>0.023071</td>\n",
       "      <td>0.024602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.819259</td>\n",
       "      <td>0.425910</td>\n",
       "      <td>0.643365</td>\n",
       "      <td>0.796421</td>\n",
       "      <td>0.541725</td>\n",
       "      <td>0.406386</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.381432</td>\n",
       "      <td>0.749537</td>\n",
       "      <td>0.681616</td>\n",
       "      <td>0.431841</td>\n",
       "      <td>0.034896</td>\n",
       "      <td>0.022318</td>\n",
       "      <td>0.023479</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.830109</td>\n",
       "      <td>0.394976</td>\n",
       "      <td>0.610503</td>\n",
       "      <td>0.772254</td>\n",
       "      <td>0.542691</td>\n",
       "      <td>0.391360</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.388568</td>\n",
       "      <td>0.753204</td>\n",
       "      <td>0.678869</td>\n",
       "      <td>0.423637</td>\n",
       "      <td>0.033992</td>\n",
       "      <td>0.020589</td>\n",
       "      <td>0.023388</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.409811</td>\n",
       "      <td>0.615017</td>\n",
       "      <td>0.769863</td>\n",
       "      <td>0.527231</td>\n",
       "      <td>0.425366</td>\n",
       "      <td>0.024923</td>\n",
       "      <td>0.391970</td>\n",
       "      <td>0.745287</td>\n",
       "      <td>0.672690</td>\n",
       "      <td>0.411734</td>\n",
       "      <td>0.033992</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.846187</td>\n",
       "      <td>0.398938</td>\n",
       "      <td>0.649406</td>\n",
       "      <td>0.779239</td>\n",
       "      <td>0.559557</td>\n",
       "      <td>0.454725</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>0.386862</td>\n",
       "      <td>0.745879</td>\n",
       "      <td>0.673033</td>\n",
       "      <td>0.418367</td>\n",
       "      <td>0.035799</td>\n",
       "      <td>0.018545</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.827669</td>\n",
       "      <td>0.436699</td>\n",
       "      <td>0.641439</td>\n",
       "      <td>0.794031</td>\n",
       "      <td>0.527846</td>\n",
       "      <td>0.422202</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.387617</td>\n",
       "      <td>0.749886</td>\n",
       "      <td>0.672633</td>\n",
       "      <td>0.419284</td>\n",
       "      <td>0.032740</td>\n",
       "      <td>0.018626</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.844096</td>\n",
       "      <td>0.408884</td>\n",
       "      <td>0.618071</td>\n",
       "      <td>0.771245</td>\n",
       "      <td>0.530481</td>\n",
       "      <td>0.425959</td>\n",
       "      <td>0.022559</td>\n",
       "      <td>0.396636</td>\n",
       "      <td>0.721207</td>\n",
       "      <td>0.657167</td>\n",
       "      <td>0.385911</td>\n",
       "      <td>0.039693</td>\n",
       "      <td>0.019601</td>\n",
       "      <td>0.020221</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.864619</td>\n",
       "      <td>0.405007</td>\n",
       "      <td>0.660758</td>\n",
       "      <td>0.775578</td>\n",
       "      <td>0.555165</td>\n",
       "      <td>0.499011</td>\n",
       "      <td>0.035138</td>\n",
       "      <td>0.409203</td>\n",
       "      <td>0.717433</td>\n",
       "      <td>0.658194</td>\n",
       "      <td>0.369349</td>\n",
       "      <td>0.035243</td>\n",
       "      <td>0.018346</td>\n",
       "      <td>0.021686</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.859216</td>\n",
       "      <td>0.364211</td>\n",
       "      <td>0.664343</td>\n",
       "      <td>0.770274</td>\n",
       "      <td>0.595309</td>\n",
       "      <td>0.483887</td>\n",
       "      <td>0.039253</td>\n",
       "      <td>0.394773</td>\n",
       "      <td>0.717899</td>\n",
       "      <td>0.643906</td>\n",
       "      <td>0.379959</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.843573</td>\n",
       "      <td>0.363874</td>\n",
       "      <td>0.660559</td>\n",
       "      <td>0.776549</td>\n",
       "      <td>0.598823</td>\n",
       "      <td>0.452254</td>\n",
       "      <td>0.027855</td>\n",
       "      <td>0.394675</td>\n",
       "      <td>0.721547</td>\n",
       "      <td>0.643602</td>\n",
       "      <td>0.381447</td>\n",
       "      <td>0.036425</td>\n",
       "      <td>0.021069</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.858301</td>\n",
       "      <td>0.402225</td>\n",
       "      <td>0.657372</td>\n",
       "      <td>0.776848</td>\n",
       "      <td>0.557537</td>\n",
       "      <td>0.484183</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>0.386578</td>\n",
       "      <td>0.729444</td>\n",
       "      <td>0.648184</td>\n",
       "      <td>0.397383</td>\n",
       "      <td>0.037399</td>\n",
       "      <td>0.016540</td>\n",
       "      <td>0.025320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.853203</td>\n",
       "      <td>0.396325</td>\n",
       "      <td>0.630419</td>\n",
       "      <td>0.768331</td>\n",
       "      <td>0.546469</td>\n",
       "      <td>0.450573</td>\n",
       "      <td>0.026011</td>\n",
       "      <td>0.390882</td>\n",
       "      <td>0.728348</td>\n",
       "      <td>0.661445</td>\n",
       "      <td>0.398481</td>\n",
       "      <td>0.038302</td>\n",
       "      <td>0.016720</td>\n",
       "      <td>0.023779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.829891</td>\n",
       "      <td>0.421696</td>\n",
       "      <td>0.640908</td>\n",
       "      <td>0.789362</td>\n",
       "      <td>0.539881</td>\n",
       "      <td>0.422697</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.398126</td>\n",
       "      <td>0.733800</td>\n",
       "      <td>0.662101</td>\n",
       "      <td>0.392474</td>\n",
       "      <td>0.037677</td>\n",
       "      <td>0.019262</td>\n",
       "      <td>0.022810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.823181</td>\n",
       "      <td>0.428018</td>\n",
       "      <td>0.624311</td>\n",
       "      <td>0.788166</td>\n",
       "      <td>0.525562</td>\n",
       "      <td>0.397489</td>\n",
       "      <td>0.015181</td>\n",
       "      <td>0.407281</td>\n",
       "      <td>0.736100</td>\n",
       "      <td>0.654325</td>\n",
       "      <td>0.378096</td>\n",
       "      <td>0.040319</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.023656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.838214</td>\n",
       "      <td>0.464683</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.789324</td>\n",
       "      <td>0.488405</td>\n",
       "      <td>0.432977</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.410350</td>\n",
       "      <td>0.731355</td>\n",
       "      <td>0.654230</td>\n",
       "      <td>0.371963</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>0.024326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.837473</td>\n",
       "      <td>0.464936</td>\n",
       "      <td>0.669588</td>\n",
       "      <td>0.805723</td>\n",
       "      <td>0.518535</td>\n",
       "      <td>0.469949</td>\n",
       "      <td>0.029179</td>\n",
       "      <td>0.421809</td>\n",
       "      <td>0.737138</td>\n",
       "      <td>0.660884</td>\n",
       "      <td>0.365010</td>\n",
       "      <td>0.041501</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.023339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.416807</td>\n",
       "      <td>0.710483</td>\n",
       "      <td>0.812521</td>\n",
       "      <td>0.590478</td>\n",
       "      <td>0.483986</td>\n",
       "      <td>0.039016</td>\n",
       "      <td>0.410428</td>\n",
       "      <td>0.741368</td>\n",
       "      <td>0.656064</td>\n",
       "      <td>0.377971</td>\n",
       "      <td>0.039693</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>0.022820</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.823922</td>\n",
       "      <td>0.410233</td>\n",
       "      <td>0.645423</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.555077</td>\n",
       "      <td>0.413306</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.407977</td>\n",
       "      <td>0.745152</td>\n",
       "      <td>0.657205</td>\n",
       "      <td>0.383602</td>\n",
       "      <td>0.037955</td>\n",
       "      <td>0.019122</td>\n",
       "      <td>0.022581</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.829935</td>\n",
       "      <td>0.414026</td>\n",
       "      <td>0.636062</td>\n",
       "      <td>0.785850</td>\n",
       "      <td>0.543306</td>\n",
       "      <td>0.416963</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.427318</td>\n",
       "      <td>0.744075</td>\n",
       "      <td>0.644296</td>\n",
       "      <td>0.352885</td>\n",
       "      <td>0.033992</td>\n",
       "      <td>0.020646</td>\n",
       "      <td>0.024223</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18215</th>\n",
       "      <td>0.803617</td>\n",
       "      <td>0.364295</td>\n",
       "      <td>0.686384</td>\n",
       "      <td>0.796683</td>\n",
       "      <td>0.638440</td>\n",
       "      <td>0.438909</td>\n",
       "      <td>0.041665</td>\n",
       "      <td>0.761521</td>\n",
       "      <td>0.570916</td>\n",
       "      <td>0.429604</td>\n",
       "      <td>0.266725</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.478106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18216</th>\n",
       "      <td>0.799346</td>\n",
       "      <td>0.360334</td>\n",
       "      <td>0.691297</td>\n",
       "      <td>0.796870</td>\n",
       "      <td>0.649947</td>\n",
       "      <td>0.437525</td>\n",
       "      <td>0.047718</td>\n",
       "      <td>0.705802</td>\n",
       "      <td>0.614526</td>\n",
       "      <td>0.485679</td>\n",
       "      <td>0.138556</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.306647</td>\n",
       "      <td>0.529996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18217</th>\n",
       "      <td>0.795904</td>\n",
       "      <td>0.361345</td>\n",
       "      <td>0.697935</td>\n",
       "      <td>0.798065</td>\n",
       "      <td>0.658995</td>\n",
       "      <td>0.430704</td>\n",
       "      <td>0.052495</td>\n",
       "      <td>0.665494</td>\n",
       "      <td>0.681934</td>\n",
       "      <td>0.525842</td>\n",
       "      <td>0.075188</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.315360</td>\n",
       "      <td>0.524834</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18218</th>\n",
       "      <td>0.793115</td>\n",
       "      <td>0.371628</td>\n",
       "      <td>0.700060</td>\n",
       "      <td>0.797243</td>\n",
       "      <td>0.659698</td>\n",
       "      <td>0.417062</td>\n",
       "      <td>0.052778</td>\n",
       "      <td>0.656995</td>\n",
       "      <td>0.662511</td>\n",
       "      <td>0.503703</td>\n",
       "      <td>0.079332</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.307373</td>\n",
       "      <td>0.547937</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18219</th>\n",
       "      <td>0.791242</td>\n",
       "      <td>0.368426</td>\n",
       "      <td>0.698865</td>\n",
       "      <td>0.795861</td>\n",
       "      <td>0.663212</td>\n",
       "      <td>0.419632</td>\n",
       "      <td>0.054339</td>\n",
       "      <td>0.636880</td>\n",
       "      <td>0.674415</td>\n",
       "      <td>0.515728</td>\n",
       "      <td>0.068889</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.307661</td>\n",
       "      <td>0.528429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18220</th>\n",
       "      <td>0.790370</td>\n",
       "      <td>0.367330</td>\n",
       "      <td>0.702251</td>\n",
       "      <td>0.796982</td>\n",
       "      <td>0.667252</td>\n",
       "      <td>0.418446</td>\n",
       "      <td>0.056656</td>\n",
       "      <td>0.647868</td>\n",
       "      <td>0.684408</td>\n",
       "      <td>0.515918</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.284057</td>\n",
       "      <td>0.482222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18221</th>\n",
       "      <td>0.791155</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.704972</td>\n",
       "      <td>0.798588</td>\n",
       "      <td>0.667077</td>\n",
       "      <td>0.415876</td>\n",
       "      <td>0.056940</td>\n",
       "      <td>0.641938</td>\n",
       "      <td>0.649210</td>\n",
       "      <td>0.566024</td>\n",
       "      <td>0.013030</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.264169</td>\n",
       "      <td>0.465303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18222</th>\n",
       "      <td>0.790414</td>\n",
       "      <td>0.375084</td>\n",
       "      <td>0.707030</td>\n",
       "      <td>0.798924</td>\n",
       "      <td>0.666725</td>\n",
       "      <td>0.408066</td>\n",
       "      <td>0.057224</td>\n",
       "      <td>0.636497</td>\n",
       "      <td>0.671417</td>\n",
       "      <td>0.543019</td>\n",
       "      <td>0.040479</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.292158</td>\n",
       "      <td>0.512860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18223</th>\n",
       "      <td>0.790240</td>\n",
       "      <td>0.382333</td>\n",
       "      <td>0.711810</td>\n",
       "      <td>0.800717</td>\n",
       "      <td>0.666725</td>\n",
       "      <td>0.398082</td>\n",
       "      <td>0.058454</td>\n",
       "      <td>0.665788</td>\n",
       "      <td>0.686164</td>\n",
       "      <td>0.547877</td>\n",
       "      <td>0.068764</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.327206</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18224</th>\n",
       "      <td>0.791329</td>\n",
       "      <td>0.378372</td>\n",
       "      <td>0.713669</td>\n",
       "      <td>0.802249</td>\n",
       "      <td>0.668306</td>\n",
       "      <td>0.402135</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>0.662563</td>\n",
       "      <td>0.679111</td>\n",
       "      <td>0.545234</td>\n",
       "      <td>0.058682</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.352121</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18225</th>\n",
       "      <td>0.793203</td>\n",
       "      <td>0.379130</td>\n",
       "      <td>0.712010</td>\n",
       "      <td>0.802398</td>\n",
       "      <td>0.664002</td>\n",
       "      <td>0.403519</td>\n",
       "      <td>0.057035</td>\n",
       "      <td>0.684383</td>\n",
       "      <td>0.675124</td>\n",
       "      <td>0.563951</td>\n",
       "      <td>0.074799</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.328461</td>\n",
       "      <td>0.556813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18226</th>\n",
       "      <td>0.795773</td>\n",
       "      <td>0.380142</td>\n",
       "      <td>0.710815</td>\n",
       "      <td>0.803183</td>\n",
       "      <td>0.658907</td>\n",
       "      <td>0.405002</td>\n",
       "      <td>0.054528</td>\n",
       "      <td>0.661112</td>\n",
       "      <td>0.662812</td>\n",
       "      <td>0.559607</td>\n",
       "      <td>0.037615</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.304170</td>\n",
       "      <td>0.519282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18227</th>\n",
       "      <td>0.795468</td>\n",
       "      <td>0.382839</td>\n",
       "      <td>0.708026</td>\n",
       "      <td>0.801651</td>\n",
       "      <td>0.656184</td>\n",
       "      <td>0.403223</td>\n",
       "      <td>0.052826</td>\n",
       "      <td>0.687579</td>\n",
       "      <td>0.699068</td>\n",
       "      <td>0.589523</td>\n",
       "      <td>0.111927</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.340707</td>\n",
       "      <td>0.579845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18228</th>\n",
       "      <td>0.795120</td>\n",
       "      <td>0.381069</td>\n",
       "      <td>0.704508</td>\n",
       "      <td>0.799933</td>\n",
       "      <td>0.655306</td>\n",
       "      <td>0.406485</td>\n",
       "      <td>0.051549</td>\n",
       "      <td>0.689020</td>\n",
       "      <td>0.682245</td>\n",
       "      <td>0.583657</td>\n",
       "      <td>0.094155</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.348890</td>\n",
       "      <td>0.576333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18229</th>\n",
       "      <td>0.795904</td>\n",
       "      <td>0.378877</td>\n",
       "      <td>0.700325</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.652319</td>\n",
       "      <td>0.411427</td>\n",
       "      <td>0.049373</td>\n",
       "      <td>0.653593</td>\n",
       "      <td>0.694188</td>\n",
       "      <td>0.565140</td>\n",
       "      <td>0.073061</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.313947</td>\n",
       "      <td>0.520349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18230</th>\n",
       "      <td>0.795338</td>\n",
       "      <td>0.362610</td>\n",
       "      <td>0.693620</td>\n",
       "      <td>0.795786</td>\n",
       "      <td>0.656360</td>\n",
       "      <td>0.431099</td>\n",
       "      <td>0.050698</td>\n",
       "      <td>0.671620</td>\n",
       "      <td>0.686426</td>\n",
       "      <td>0.551118</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.339885</td>\n",
       "      <td>0.607279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18231</th>\n",
       "      <td>0.793725</td>\n",
       "      <td>0.348028</td>\n",
       "      <td>0.695014</td>\n",
       "      <td>0.796123</td>\n",
       "      <td>0.666550</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.056798</td>\n",
       "      <td>0.666405</td>\n",
       "      <td>0.691287</td>\n",
       "      <td>0.565330</td>\n",
       "      <td>0.075981</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.337046</td>\n",
       "      <td>0.608917</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18232</th>\n",
       "      <td>0.794031</td>\n",
       "      <td>0.342802</td>\n",
       "      <td>0.696674</td>\n",
       "      <td>0.797206</td>\n",
       "      <td>0.669712</td>\n",
       "      <td>0.449288</td>\n",
       "      <td>0.059021</td>\n",
       "      <td>0.670865</td>\n",
       "      <td>0.689007</td>\n",
       "      <td>0.556366</td>\n",
       "      <td>0.075188</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.347978</td>\n",
       "      <td>0.585599</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18233</th>\n",
       "      <td>0.795643</td>\n",
       "      <td>0.333783</td>\n",
       "      <td>0.697736</td>\n",
       "      <td>0.798775</td>\n",
       "      <td>0.672259</td>\n",
       "      <td>0.459371</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.639066</td>\n",
       "      <td>0.683671</td>\n",
       "      <td>0.542829</td>\n",
       "      <td>0.056888</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.345921</td>\n",
       "      <td>0.593670</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18234</th>\n",
       "      <td>0.797473</td>\n",
       "      <td>0.322151</td>\n",
       "      <td>0.701255</td>\n",
       "      <td>0.801726</td>\n",
       "      <td>0.677266</td>\n",
       "      <td>0.471135</td>\n",
       "      <td>0.066399</td>\n",
       "      <td>0.630704</td>\n",
       "      <td>0.676045</td>\n",
       "      <td>0.548913</td>\n",
       "      <td>0.045569</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.361897</td>\n",
       "      <td>0.639151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18235</th>\n",
       "      <td>0.797298</td>\n",
       "      <td>0.318358</td>\n",
       "      <td>0.704043</td>\n",
       "      <td>0.803033</td>\n",
       "      <td>0.681219</td>\n",
       "      <td>0.473606</td>\n",
       "      <td>0.069047</td>\n",
       "      <td>0.629969</td>\n",
       "      <td>0.645678</td>\n",
       "      <td>0.526080</td>\n",
       "      <td>0.039562</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.438202</td>\n",
       "      <td>0.694044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18236</th>\n",
       "      <td>0.797298</td>\n",
       "      <td>0.320634</td>\n",
       "      <td>0.712076</td>\n",
       "      <td>0.806544</td>\n",
       "      <td>0.685436</td>\n",
       "      <td>0.467378</td>\n",
       "      <td>0.071412</td>\n",
       "      <td>0.622127</td>\n",
       "      <td>0.683651</td>\n",
       "      <td>0.524711</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.380985</td>\n",
       "      <td>0.638867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18237</th>\n",
       "      <td>0.798344</td>\n",
       "      <td>0.326197</td>\n",
       "      <td>0.717653</td>\n",
       "      <td>0.809421</td>\n",
       "      <td>0.685084</td>\n",
       "      <td>0.459668</td>\n",
       "      <td>0.071317</td>\n",
       "      <td>0.633958</td>\n",
       "      <td>0.687668</td>\n",
       "      <td>0.528219</td>\n",
       "      <td>0.071392</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.374850</td>\n",
       "      <td>0.640197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18238</th>\n",
       "      <td>0.801264</td>\n",
       "      <td>0.328810</td>\n",
       "      <td>0.720109</td>\n",
       "      <td>0.811923</td>\n",
       "      <td>0.681044</td>\n",
       "      <td>0.457888</td>\n",
       "      <td>0.070229</td>\n",
       "      <td>0.644163</td>\n",
       "      <td>0.713378</td>\n",
       "      <td>0.523893</td>\n",
       "      <td>0.106865</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.422652</td>\n",
       "      <td>0.686060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18239</th>\n",
       "      <td>0.803704</td>\n",
       "      <td>0.330496</td>\n",
       "      <td>0.721370</td>\n",
       "      <td>0.813679</td>\n",
       "      <td>0.677442</td>\n",
       "      <td>0.457295</td>\n",
       "      <td>0.069236</td>\n",
       "      <td>0.609187</td>\n",
       "      <td>0.712262</td>\n",
       "      <td>0.533799</td>\n",
       "      <td>0.108256</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.397196</td>\n",
       "      <td>0.640232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18240</th>\n",
       "      <td>0.807102</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.723760</td>\n",
       "      <td>0.816481</td>\n",
       "      <td>0.674104</td>\n",
       "      <td>0.458877</td>\n",
       "      <td>0.069284</td>\n",
       "      <td>0.612775</td>\n",
       "      <td>0.723555</td>\n",
       "      <td>0.548153</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.406079</td>\n",
       "      <td>0.665756</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18241</th>\n",
       "      <td>0.808017</td>\n",
       "      <td>0.333109</td>\n",
       "      <td>0.721370</td>\n",
       "      <td>0.815771</td>\n",
       "      <td>0.669888</td>\n",
       "      <td>0.457691</td>\n",
       "      <td>0.066824</td>\n",
       "      <td>0.614206</td>\n",
       "      <td>0.697748</td>\n",
       "      <td>0.521431</td>\n",
       "      <td>0.095031</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.406238</td>\n",
       "      <td>0.668010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18242</th>\n",
       "      <td>0.807930</td>\n",
       "      <td>0.337492</td>\n",
       "      <td>0.723362</td>\n",
       "      <td>0.816481</td>\n",
       "      <td>0.669185</td>\n",
       "      <td>0.452056</td>\n",
       "      <td>0.066351</td>\n",
       "      <td>0.599512</td>\n",
       "      <td>0.740262</td>\n",
       "      <td>0.501126</td>\n",
       "      <td>0.165672</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.398277</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>0.808235</td>\n",
       "      <td>0.344066</td>\n",
       "      <td>0.723827</td>\n",
       "      <td>0.816593</td>\n",
       "      <td>0.665935</td>\n",
       "      <td>0.445235</td>\n",
       "      <td>0.064365</td>\n",
       "      <td>0.616529</td>\n",
       "      <td>0.719131</td>\n",
       "      <td>0.501887</td>\n",
       "      <td>0.133689</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.399384</td>\n",
       "      <td>0.653724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>0.809412</td>\n",
       "      <td>0.350809</td>\n",
       "      <td>0.719578</td>\n",
       "      <td>0.815061</td>\n",
       "      <td>0.658204</td>\n",
       "      <td>0.441083</td>\n",
       "      <td>0.059541</td>\n",
       "      <td>0.671846</td>\n",
       "      <td>0.724186</td>\n",
       "      <td>0.505461</td>\n",
       "      <td>0.140517</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.385716</td>\n",
       "      <td>0.649500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18244 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
       "1       0.830850   0.446224   0.631946   0.791192   0.511947   0.421807   \n",
       "2       0.826580   0.436446   0.646286   0.796235   0.531711   0.424278   \n",
       "3       0.831329   0.443695   0.633937   0.791117   0.515285   0.423883   \n",
       "4       0.836035   0.444538   0.636195   0.789810   0.514406   0.434361   \n",
       "5       0.831590   0.419504   0.649007   0.790930   0.546557   0.432187   \n",
       "6       0.841177   0.433075   0.639979   0.786037   0.525035   0.444543   \n",
       "7       0.835338   0.480529   0.612162   0.789660   0.466883   0.420127   \n",
       "8       0.832288   0.481035   0.614353   0.792051   0.469079   0.416469   \n",
       "9       0.827930   0.465273   0.611830   0.789773   0.482783   0.403124   \n",
       "10      0.839129   0.482468   0.599549   0.783721   0.455288   0.416469   \n",
       "11      0.851155   0.450270   0.638452   0.784506   0.505358   0.464907   \n",
       "12      0.828715   0.450691   0.634800   0.794292   0.510717   0.421313   \n",
       "13      0.819259   0.425910   0.643365   0.796421   0.541725   0.406386   \n",
       "14      0.830109   0.394976   0.610503   0.772254   0.542691   0.391360   \n",
       "15      0.845098   0.409811   0.615017   0.769863   0.527231   0.425366   \n",
       "16      0.846187   0.398938   0.649406   0.779239   0.559557   0.454725   \n",
       "17      0.827669   0.436699   0.641439   0.794031   0.527846   0.422202   \n",
       "18      0.844096   0.408884   0.618071   0.771245   0.530481   0.425959   \n",
       "19      0.864619   0.405007   0.660758   0.775578   0.555165   0.499011   \n",
       "20      0.859216   0.364211   0.664343   0.770274   0.595309   0.483887   \n",
       "21      0.843573   0.363874   0.660559   0.776549   0.598823   0.452254   \n",
       "22      0.858301   0.402225   0.657372   0.776848   0.557537   0.484183   \n",
       "23      0.853203   0.396325   0.630419   0.768331   0.546469   0.450573   \n",
       "24      0.829891   0.421696   0.640908   0.789362   0.539881   0.422697   \n",
       "25      0.823181   0.428018   0.624311   0.788166   0.525562   0.397489   \n",
       "26      0.838214   0.464683   0.625108   0.789324   0.488405   0.432977   \n",
       "27      0.837473   0.464936   0.669588   0.805723   0.518535   0.469949   \n",
       "28      0.831373   0.416807   0.710483   0.812521   0.590478   0.483986   \n",
       "29      0.823922   0.410233   0.645423   0.791304   0.555077   0.413306   \n",
       "30      0.829935   0.414026   0.636062   0.785850   0.543306   0.416963   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "18215   0.803617   0.364295   0.686384   0.796683   0.638440   0.438909   \n",
       "18216   0.799346   0.360334   0.691297   0.796870   0.649947   0.437525   \n",
       "18217   0.795904   0.361345   0.697935   0.798065   0.658995   0.430704   \n",
       "18218   0.793115   0.371628   0.700060   0.797243   0.659698   0.417062   \n",
       "18219   0.791242   0.368426   0.698865   0.795861   0.663212   0.419632   \n",
       "18220   0.790370   0.367330   0.702251   0.796982   0.667252   0.418446   \n",
       "18221   0.791155   0.369100   0.704972   0.798588   0.667077   0.415876   \n",
       "18222   0.790414   0.375084   0.707030   0.798924   0.666725   0.408066   \n",
       "18223   0.790240   0.382333   0.711810   0.800717   0.666725   0.398082   \n",
       "18224   0.791329   0.378372   0.713669   0.802249   0.668306   0.402135   \n",
       "18225   0.793203   0.379130   0.712010   0.802398   0.664002   0.403519   \n",
       "18226   0.795773   0.380142   0.710815   0.803183   0.658907   0.405002   \n",
       "18227   0.795468   0.382839   0.708026   0.801651   0.656184   0.403223   \n",
       "18228   0.795120   0.381069   0.704508   0.799933   0.655306   0.406485   \n",
       "18229   0.795904   0.378877   0.700325   0.798551   0.652319   0.411427   \n",
       "18230   0.795338   0.362610   0.693620   0.795786   0.656360   0.431099   \n",
       "18231   0.793725   0.348028   0.695014   0.796123   0.666550   0.444444   \n",
       "18232   0.794031   0.342802   0.696674   0.797206   0.669712   0.449288   \n",
       "18233   0.795643   0.333783   0.697736   0.798775   0.672259   0.459371   \n",
       "18234   0.797473   0.322151   0.701255   0.801726   0.677266   0.471135   \n",
       "18235   0.797298   0.318358   0.704043   0.803033   0.681219   0.473606   \n",
       "18236   0.797298   0.320634   0.712076   0.806544   0.685436   0.467378   \n",
       "18237   0.798344   0.326197   0.717653   0.809421   0.685084   0.459668   \n",
       "18238   0.801264   0.328810   0.720109   0.811923   0.681044   0.457888   \n",
       "18239   0.803704   0.330496   0.721370   0.813679   0.677442   0.457295   \n",
       "18240   0.807102   0.330243   0.723760   0.816481   0.674104   0.458877   \n",
       "18241   0.808017   0.333109   0.721370   0.815771   0.669888   0.457691   \n",
       "18242   0.807930   0.337492   0.723362   0.816481   0.669185   0.452056   \n",
       "18243   0.808235   0.344066   0.723827   0.816593   0.665935   0.445235   \n",
       "18244   0.809412   0.350809   0.719578   0.815061   0.658204   0.441083   \n",
       "\n",
       "       var7(t-1)  var8(t-1)  var9(t-1)  var10(t-1)  var11(t-1)  var12(t-1)  \\\n",
       "1       0.017356   0.431023   0.764555    0.645218    0.362104    0.038302   \n",
       "2       0.008560   0.438072   0.759976    0.642756    0.349659    0.045255   \n",
       "3       0.015512   0.434592   0.757677    0.640769    0.351063    0.042752   \n",
       "4       0.016505   0.437709   0.762431    0.642994    0.351884    0.042126   \n",
       "5       0.000000   0.431161   0.758783    0.646140    0.358530    0.041501   \n",
       "6       0.013809   0.415143   0.777983    0.647804    0.390819    0.038302   \n",
       "7       0.041617   0.402939   0.798085    0.643593    0.417379    0.035521   \n",
       "8       0.040624   0.391617   0.786278    0.645674    0.422677    0.033366   \n",
       "9       0.034476   0.390548   0.786123    0.649163    0.425417    0.034618   \n",
       "10      0.048759   0.388490   0.777546    0.643593    0.419549    0.031836   \n",
       "11      0.028092   0.388568   0.763954    0.656863    0.417282    0.034896   \n",
       "12      0.018491   0.382549   0.742028    0.684259    0.428504    0.035521   \n",
       "13      0.008040   0.381432   0.749537    0.681616    0.431841    0.034896   \n",
       "14      0.023930   0.388568   0.753204    0.678869    0.423637    0.033992   \n",
       "15      0.024923   0.391970   0.745287    0.672690    0.411734    0.033992   \n",
       "16      0.015465   0.386862   0.745879    0.673033    0.418367    0.035799   \n",
       "17      0.009459   0.387617   0.749886    0.672633    0.419284    0.032740   \n",
       "18      0.022559   0.396636   0.721207    0.657167    0.385911    0.039693   \n",
       "19      0.035138   0.409203   0.717433    0.658194    0.369349    0.035243   \n",
       "20      0.039253   0.394773   0.717899    0.643906    0.379959    0.036773   \n",
       "21      0.027855   0.394675   0.721547    0.643602    0.381447    0.036425   \n",
       "22      0.028234   0.386578   0.729444    0.648184    0.397383    0.037399   \n",
       "23      0.026011   0.390882   0.728348    0.661445    0.398481    0.038302   \n",
       "24      0.002601   0.398126   0.733800    0.662101    0.392474    0.037677   \n",
       "25      0.015181   0.407281   0.736100    0.654325    0.378096    0.040319   \n",
       "26      0.030220   0.410350   0.731355    0.654230    0.371963    0.042752   \n",
       "27      0.029179   0.421809   0.737138    0.660884    0.365010    0.041501   \n",
       "28      0.039016   0.410428   0.741368    0.656064    0.377971    0.039693   \n",
       "29      0.000615   0.407977   0.745152    0.657205    0.383602    0.037955   \n",
       "30      0.004067   0.427318   0.744075    0.644296    0.352885    0.033992   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "18215   0.041665   0.761521   0.570916    0.429604    0.266725    0.001662   \n",
       "18216   0.047718   0.705802   0.614526    0.485679    0.138556    0.001384   \n",
       "18217   0.052495   0.665494   0.681934    0.525842    0.075188    0.001314   \n",
       "18218   0.052778   0.656995   0.662511    0.503703    0.079332    0.001314   \n",
       "18219   0.054339   0.636880   0.674415    0.515728    0.068889    0.001314   \n",
       "18220   0.056656   0.647868   0.684408    0.515918    0.079165    0.001384   \n",
       "18221   0.056940   0.641938   0.649210    0.566024    0.013030    0.001384   \n",
       "18222   0.057224   0.636497   0.671417    0.543019    0.040479    0.001314   \n",
       "18223   0.058454   0.665788   0.686164    0.547877    0.068764    0.001105   \n",
       "18224   0.059305   0.662563   0.679111    0.545234    0.058682    0.001036   \n",
       "18225   0.057035   0.684383   0.675124    0.563951    0.074799    0.001175   \n",
       "18226   0.054528   0.661112   0.662812    0.559607    0.037615    0.001175   \n",
       "18227   0.052826   0.687579   0.699068    0.589523    0.111927    0.001105   \n",
       "18228   0.051549   0.689020   0.682245    0.583657    0.094155    0.001105   \n",
       "18229   0.049373   0.653593   0.694188    0.565140    0.073061    0.001245   \n",
       "18230   0.050698   0.671620   0.686426    0.551118    0.073005    0.001105   \n",
       "18231   0.056798   0.666405   0.691287    0.565330    0.075981    0.001036   \n",
       "18232   0.059021   0.670865   0.689007    0.556366    0.075188    0.000966   \n",
       "18233   0.061622   0.639066   0.683671    0.542829    0.056888    0.001036   \n",
       "18234   0.066399   0.630704   0.676045    0.548913    0.045569    0.000897   \n",
       "18235   0.069047   0.629969   0.645678    0.526080    0.039562    0.000758   \n",
       "18236   0.071412   0.622127   0.683651    0.524711    0.073005    0.000827   \n",
       "18237   0.071317   0.633958   0.687668    0.528219    0.071392    0.000827   \n",
       "18238   0.070229   0.644163   0.713378    0.523893    0.106865    0.000758   \n",
       "18239   0.069236   0.609187   0.712262    0.533799    0.108256    0.000827   \n",
       "18240   0.069284   0.612775   0.723555    0.548153    0.117600    0.000827   \n",
       "18241   0.066824   0.614206   0.697748    0.521431    0.095031    0.000897   \n",
       "18242   0.066351   0.599512   0.740262    0.501126    0.165672    0.000897   \n",
       "18243   0.064365   0.616529   0.719131    0.501887    0.133689    0.000897   \n",
       "18244   0.059541   0.671846   0.724186    0.505461    0.140517    0.000966   \n",
       "\n",
       "       var13(t-1)  var14(t-1)  var15(t-1)  var15(t)  \n",
       "1        0.013629    0.020626         1.0       1.0  \n",
       "2        0.013365    0.020129         1.0       1.0  \n",
       "3        0.013923    0.019825         1.0       1.0  \n",
       "4        0.014391    0.021605         1.0       1.0  \n",
       "5        0.018190    0.020026         1.0       1.0  \n",
       "6        0.019689    0.020348         1.0       1.0  \n",
       "7        0.019466    0.021215         1.0       1.0  \n",
       "8        0.019659    0.021469         1.0       1.0  \n",
       "9        0.017506    0.020604         1.0       1.0  \n",
       "10       0.015895    0.024456         1.0       1.0  \n",
       "11       0.019955    0.022581         1.0       1.0  \n",
       "12       0.023071    0.024602         1.0       1.0  \n",
       "13       0.022318    0.023479         1.0       1.0  \n",
       "14       0.020589    0.023388         1.0       1.0  \n",
       "15       0.016216    0.025691         1.0       1.0  \n",
       "16       0.018545    0.023336         1.0       1.0  \n",
       "17       0.018626    0.022692         1.0       1.0  \n",
       "18       0.019601    0.020221         1.0       1.0  \n",
       "19       0.018346    0.021686         1.0       1.0  \n",
       "20       0.014556    0.022727         1.0       1.0  \n",
       "21       0.021069    0.022177         1.0       1.0  \n",
       "22       0.016540    0.025320         1.0       1.0  \n",
       "23       0.016720    0.023779         1.0       1.0  \n",
       "24       0.019262    0.022810         1.0       1.0  \n",
       "25       0.019566    0.023656         1.0       1.0  \n",
       "26       0.016706    0.024326         1.0       1.0  \n",
       "27       0.018926    0.023339         1.0       1.0  \n",
       "28       0.015682    0.022820         1.0       1.0  \n",
       "29       0.019122    0.022581         1.0       1.0  \n",
       "30       0.020646    0.024223         1.0       1.0  \n",
       "...           ...         ...         ...       ...  \n",
       "18215    0.279700    0.478106         1.0       1.0  \n",
       "18216    0.306647    0.529996         1.0       1.0  \n",
       "18217    0.315360    0.524834         1.0       1.0  \n",
       "18218    0.307373    0.547937         1.0       1.0  \n",
       "18219    0.307661    0.528429         1.0       1.0  \n",
       "18220    0.284057    0.482222         1.0       1.0  \n",
       "18221    0.264169    0.465303         1.0       1.0  \n",
       "18222    0.292158    0.512860         1.0       1.0  \n",
       "18223    0.327206    0.575290         1.0       1.0  \n",
       "18224    0.352121    0.595388         1.0       1.0  \n",
       "18225    0.328461    0.556813         1.0       1.0  \n",
       "18226    0.304170    0.519282         1.0       1.0  \n",
       "18227    0.340707    0.579845         1.0       1.0  \n",
       "18228    0.348890    0.576333         1.0       1.0  \n",
       "18229    0.313947    0.520349         1.0       1.0  \n",
       "18230    0.339885    0.607279         1.0       1.0  \n",
       "18231    0.337046    0.608917         1.0       1.0  \n",
       "18232    0.347978    0.585599         1.0       1.0  \n",
       "18233    0.345921    0.593670         1.0       1.0  \n",
       "18234    0.361897    0.639151         1.0       1.0  \n",
       "18235    0.438202    0.694044         1.0       1.0  \n",
       "18236    0.380985    0.638867         1.0       1.0  \n",
       "18237    0.374850    0.640197         1.0       1.0  \n",
       "18238    0.422652    0.686060         1.0       1.0  \n",
       "18239    0.397196    0.640232         1.0       1.0  \n",
       "18240    0.406079    0.665756         1.0       1.0  \n",
       "18241    0.406238    0.668010         1.0       1.0  \n",
       "18242    0.398277    0.667738         1.0       1.0  \n",
       "18243    0.399384    0.653724         1.0       1.0  \n",
       "18244    0.385716    0.649500         1.0       1.0  \n",
       "\n",
       "[18244 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into train/dev and test sets.\n",
    "\n",
    "We use 60% of the data for training, of which 20% is used for evaluation purposes (to make sure we are not overfitting). The rest of the data (40%) will be set aside to test the accuracy of the model.\n",
    "\n",
    "Note: we could also use 50% or 40% of the data for training/development purposes and use the remaining 50% or 60% of the data for test. The outcome would be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape = (10858, 1, 16)\n",
      "test_X.shape  = (7386, 1, 16)\n",
      "train_Y.shape = (10858, 3)\n",
      "test_Y.shape  = (7386, 3)\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# splitting the data into train and test sets\n",
    "\n",
    "# taking 60% of the data for train/dev, and the remaining 40% for test\n",
    "n_train = 89 * int( reframed.values.shape[0]/89 * 0.60) #each time series has 89 entities\n",
    "\n",
    "# inputs\n",
    "train_X = np.array(reframed.values[:n_train, :])\n",
    "test_X  = np.array(reframed.values[n_train:, :])\n",
    "# outpts (making sure outputs are arrays of integers)\n",
    "last_column = values.shape[1] - 1 # last column (goal) in the raw dataset\n",
    "train_Y = np.array(values[t_prev:n_train+t_prev, last_column], dtype=int)\n",
    "test_Y  = np.array(values[t_prev+n_train:   , last_column], dtype=int)\n",
    "\n",
    "# Lets convert Y into a \"one-hot representation\" to make it suitable for the softmax classifier.\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "train_Y = convert_to_one_hot(train_Y, C = 3)\n",
    "test_Y  = convert_to_one_hot(test_Y,  C = 3)\n",
    "# now each row in Y will be a one-hot vector representing each class (0, 1, or 2)\n",
    "\n",
    "# reshaping the input data to be 3D for Keras [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], t_prev, train_X.shape[1]))\n",
    "test_X  = test_X.reshape( (test_X.shape[0],  t_prev,  test_X.shape[1]))\n",
    "print(\"train_X.shape =\", train_X.shape)\n",
    "print(\"test_X.shape  =\", test_X.shape)\n",
    "print(\"train_Y.shape =\", train_Y.shape)\n",
    "print(\"test_Y.shape  =\", test_Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will design the LSTM model in Keras. We use 32 hidden units for the LSTM layer. For optimization, we use Adam algorithm with a learning rate of 0.01, $\\beta_1$=0.9, $\\beta_2$=0.999, and a decay rate of 0.01. We set the number of epuchs to 4 (higher values result in overfitting).\n",
    "\n",
    "#### The most important hyperparameters in our model are:\n",
    "- number of hidden units in LSTM\n",
    "- learning rate\n",
    "- timestep (t_prev)\n",
    "- number of epuchs\n",
    "\n",
    "The values for the chosen hyperparameters above lead to satisfactory results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8686 samples, validate on 2172 samples\n",
      "Epoch 1/4\n",
      " - 2s - loss: 0.7950 - acc: 0.6746 - val_loss: 0.4237 - val_acc: 0.6994\n",
      "Epoch 2/4\n",
      " - 1s - loss: 0.2263 - acc: 0.9557 - val_loss: 0.1646 - val_acc: 0.9862\n",
      "Epoch 3/4\n",
      " - 1s - loss: 0.0870 - acc: 0.9968 - val_loss: 0.0863 - val_acc: 0.9963\n",
      "Epoch 4/4\n",
      " - 1s - loss: 0.0520 - acc: 0.9965 - val_loss: 0.0584 - val_acc: 0.9963\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 6,371\n",
      "Trainable params: 6,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# building the model in Keras\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "\n",
    "# designing the network\n",
    "input_shape = (train_X.shape[1], train_X.shape[2])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=input_shape))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "### optimizer='adam'\n",
    "### loss='categorical_accuracy', 'msle'\n",
    "### metrics=['accuracy'], ['msle', 'mae']\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fitting the network\n",
    "#Trained = model.fit(train_X, train_Y, epochs=5, validation_data=(test_X, test_Y), verbose=2, shuffle=False)\n",
    "Trained = model.fit(train_X, train_Y, epochs=4, validation_split = 0.2, verbose=2, shuffle=False)\n",
    "\n",
    "# plotting history\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Loss')\n",
    "pyplot.plot(Trained.history['loss'],     label='train')\n",
    "pyplot.plot(Trained.history['val_loss'], label='eval')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the training and development sets, based on the categorical crossentropy loss, is found to be 99.71% and 99.63%, respectively.\n",
    "\n",
    "We now evaluate the model to test its accuracy and to create a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7386/7386 [==============================] - 0s 51us/step\n",
      "\n",
      "Test Loss:  0.05431774335990529\n",
      "Accuracy:   0.9959382615759546\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3364   17    0]\n",
      " [   0 1864    5]\n",
      " [   0    8 2128]]\n",
      "\n",
      " Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No event 0 (-)       1.00      0.99      1.00      3381\n",
      "   Event 1 (+)       0.99      1.00      0.99      1869\n",
      "   Event 2 (+)       1.00      1.00      1.00      2136\n",
      "\n",
      "     micro avg       1.00      1.00      1.00      7386\n",
      "     macro avg       0.99      1.00      1.00      7386\n",
      "  weighted avg       1.00      1.00      1.00      7386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# evaluating the model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "Test = model.evaluate( test_X, test_Y, verbose=1 )\n",
    "print()\n",
    "print(\"Test Loss: \", Test[0])\n",
    "print(\"Accuracy:  \", Test[1])\n",
    "print()\n",
    "\n",
    "Predict = model.predict(test_X)\n",
    "#print(\"Predict.shape =\", Predict.shape)\n",
    "\n",
    "Y_pred = np.argmax(Predict, axis=1)\n",
    "Y_true = np.argmax(test_Y,  axis=1)\n",
    "\n",
    "#for i in range(150,160):\n",
    "#    print(Y_pred[i], Y_true[i])\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_true, Y_pred))\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "target_classes = ['No event 0 (-)', 'Event 1 (+)', 'Event 2 (+)']\n",
    "print(classification_report(Y_true, Y_pred, target_names=target_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the overal accuracy and the confusion matrix, other important criteria such as precision and recall of our classifier are shown in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
