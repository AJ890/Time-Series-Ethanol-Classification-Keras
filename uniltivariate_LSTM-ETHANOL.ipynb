{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier model for the Multivariate Time Series data using LSTM in Keras\n",
    "\n",
    "\n",
    "This is a three-class classification problem for the given multivariate time series data. The \"goal\" column has three values 0 (no event), 1 (event of type 1), and 2 (event of type 2). Although we can treat events 1 and 2 as one \"positive\" class and 0 as the \"negative\" class, we will build a 3-class classifier and create a confusion matrix.  \n",
    "\n",
    "We use LSTM, which is a variation of RNN, for building our network.\n",
    "\n",
    "#### About the dataset:\n",
    "There are 17 columns in total, of which the first two columns are the IDs for each timeseries (to mark the beginning and end of each timeseries), and are therefore discarded. The next 14 columns are the variables listed in time in equal intervals, and the last column is the \"goal\" column. There are 89 samples in each time series. For example, rows 2 to 90 represent one time series, rows 91 to 178 represent the next time series, and so on.\n",
    "\n",
    "\n",
    "#### Lets start by importing the necessary modules and loading the raw dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# univariate lstm example\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "from pandas import read_excel, DataFrame, concat\n",
    "# split a univariate sequence into samples\n",
    "from iteration_utilities import deepflatten\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv('Ethanol.csv', header = None)\n",
    "values = df.values\n",
    "\n",
    "\n",
    "\n",
    "Nc = values.shape[1] # number of columns\n",
    "\n",
    "values = values.astype('float32') # ensuring all the data is float\n",
    "\n",
    "x_Train = df.drop(df.columns[[0]], axis=1)\n",
    "y_Train = df[df.columns[[0]]].values\n",
    "mylist = []\n",
    "for i in range(0,x_Train.shape[0]):\n",
    "    mylist.append(np.ones(x_Train.shape[1])*y_Train[i])\n",
    "mylist = pd.DataFrame(mylist)\n",
    "mylist = pd.DataFrame(mylist.stack().to_frame().values)\n",
    "\n",
    "df2 = pd.DataFrame(x_Train.stack().to_frame().values)\n",
    "scaled = pd.concat([df2, mylist], axis=1)\n",
    "values=scaled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1758004, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we normalized all the features so that they range between 0 and 1.\n",
    "\n",
    "Now we need to frame the raw dataset into a supervised learning problem.\n",
    "We set timestep equal to 1 (t_prev = 1), which means that we only look at the data in the previous row to train and predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reframed.shape before drop = (1758003, 4)\n",
      "reframed.shape after  drop = (1758003, 1)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# framing the problem as supervised learning\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame the time series as a supervised learning dataset.\n",
    "    \n",
    "    Arguments:\n",
    "        data:    Sequence of observations as a list or NumPy array.\n",
    "        n_in:    Number of lag observations as input (X).\n",
    "        n_out:   Number of observations as output (y).\n",
    "        dropnan: Boolean, whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # future sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n",
    "\n",
    "t_prev = 1 # timestep\n",
    "t_next = 1\n",
    "reframed = series_to_supervised(scaled, t_prev, t_next)\n",
    "print(\"reframed.shape before drop =\", reframed.shape)\n",
    "# dropping the columns that we don't want to predict\n",
    "reframed.drop(reframed.columns[[1,2,3]], axis=1, inplace=True)\n",
    "print(\"reframed.shape after  drop =\", reframed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var1(t-1)\n",
       "1     -0.936\n",
       "2     -0.936\n",
       "3     -0.936\n",
       "4     -0.936\n",
       "5     -0.936"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into train/dev and test sets.\n",
    "\n",
    "We use 60% of the data for training, of which 20% is used for evaluation purposes (to make sure we are not overfitting). The rest of the data (40%) will be set aside to test the accuracy of the model.\n",
    "\n",
    "Note: we could also use 50% or 40% of the data for training/development purposes and use the remaining 50% or 60% of the data for test. The outcome would be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape = (1054102, 1, 1)\n",
      "test_X.shape  = (703901, 1, 1)\n",
      "train_Y.shape = (1054102, 4)\n",
      "test_Y.shape  = (703901, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\usuario\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\users\\usuario\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# splitting the data into train and test sets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one =  OneHotEncoder()\n",
    "\n",
    "\n",
    "# taking 60% of the data for train/dev, and the remaining 40% for test\n",
    "n_train = 1751 * int( reframed.values.shape[0]/1751 * 0.60) #each time series has 89 entities\n",
    "\n",
    "# inputs\n",
    "train_X = np.array(reframed.values[:n_train, :])\n",
    "test_X  = np.array(reframed.values[n_train:, :])\n",
    "# outpts (making sure outputs are arrays of integers)\n",
    "last_column = values.shape[1] - 1 # last column (goal) in the raw dataset\n",
    "\n",
    "train_Y = np.array(values[t_prev:n_train+t_prev, last_column], dtype=int)\n",
    "test_Y  = np.array(values[t_prev+n_train:   , last_column], dtype=int)\n",
    "\n",
    "# Lets convert Y into a \"one-hot representation\" to make it suitable for the softmax classifier.\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "train_Y = one.fit_transform(train_Y.reshape(-1, 1)).toarray()\n",
    "test_Y  = one.fit_transform(test_Y.reshape(-1, 1)).toarray();\n",
    "# now each row in Y will be a one-hot vector representing each class (0, 1, or 2)\n",
    "\n",
    "# reshaping the input data to be 3D for Keras [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], t_prev, train_X.shape[1]))\n",
    "test_X  = test_X.reshape( (test_X.shape[0],  t_prev,  test_X.shape[1]))\n",
    "print(\"train_X.shape =\", train_X.shape)\n",
    "print(\"test_X.shape  =\", test_X.shape)\n",
    "print(\"train_Y.shape =\", train_Y.shape)\n",
    "print(\"test_Y.shape  =\", test_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will design the LSTM model in Keras. We use 32 hidden units for the LSTM layer. For optimization, we use Adam algorithm with a learning rate of 0.01, $\\beta_1$=0.9, $\\beta_2$=0.999, and a decay rate of 0.01. We set the number of epuchs to 4 (higher values result in overfitting).\n",
    "\n",
    "#### The most important hyperparameters in our model are:\n",
    "- number of hidden units in LSTM\n",
    "- learning rate\n",
    "- timestep (t_prev)\n",
    "- number of epuchs\n",
    "\n",
    "The values for the chosen hyperparameters above lead to satisfactory results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 843281 samples, validate on 210821 samples\n",
      "Epoch 1/50\n",
      " - 36s - loss: 1.1319 - acc: 0.5404 - val_loss: 2.0786 - val_acc: 0.2276\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-04bca4855662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# fitting the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#Trained = model.fit(train_X, train_Y, epochs=5, validation_data=(test_X, test_Y), verbose=2, shuffle=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mTrained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# plotting history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\usuario\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\usuario\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\usuario\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\usuario\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\usuario\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# building the model in Keras\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "\n",
    "# designing the network\n",
    "input_shape = (train_X.shape[1], train_X.shape[2])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu',input_shape=input_shape))\n",
    "#model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "#lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01\n",
    "opt = optimizers.Adam()\n",
    "### optimizer='adam'\n",
    "### loss='categorical_accuracy', 'msle'\n",
    "### metrics=['accuracy'], ['msle', 'mae']\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fitting the network\n",
    "#Trained = model.fit(train_X, train_Y, epochs=5, validation_data=(test_X, test_Y), verbose=2, shuffle=False)\n",
    "Trained = model.fit(train_X, train_Y, epochs=50, validation_split = 0.2, verbose=2, shuffle=False)\n",
    "\n",
    "# plotting history\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Loss')\n",
    "pyplot.plot(Trained.history['loss'],     label='train')\n",
    "pyplot.plot(Trained.history['val_loss'], label='eval')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the training and development sets, based on the categorical crossentropy loss, is found to be 99.71% and 99.63%, respectively.\n",
    "\n",
    "We now evaluate the model to test its accuracy and to create a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703901/703901 [==============================] - 9s 12us/step\n",
      "\n",
      "Test Loss:  0.25346760086386555\n",
      "Accuracy:   0.9999374912097013\n",
      "\n",
      "Confusion Matrix:\n",
      "[[173338      0      0     11]\n",
      " [    11 173338      0      0]\n",
      " [     0     11 173338      0]\n",
      " [     0      0     11 183843]]\n",
      "\n",
      " Classification Report:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 4, does not match size of target_names, 3. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-cd438d5e64c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n Classification Report:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mtarget_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'No event 0 (-)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Event 1 (+)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Event 2 (+)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\usuario\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[0;32m   1541\u001b[0m                 \u001b[1;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m                 \u001b[1;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1543\u001b[1;33m                 \u001b[1;34m\"parameter\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1544\u001b[0m             )\n\u001b[0;32m   1545\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 4, does not match size of target_names, 3. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# evaluating the model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "Test = model.evaluate( test_X, test_Y, verbose=1 )\n",
    "print()\n",
    "print(\"Test Loss: \", Test[0])\n",
    "print(\"Accuracy:  \", Test[1])\n",
    "print()\n",
    "\n",
    "Predict = model.predict(test_X)\n",
    "#print(\"Predict.shape =\", Predict.shape)\n",
    "\n",
    "Y_pred = np.argmax(Predict, axis=1)\n",
    "Y_true = np.argmax(test_Y,  axis=1)\n",
    "\n",
    "#for i in range(150,160):\n",
    "#    print(Y_pred[i], Y_true[i])\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_true, Y_pred))\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "target_classes = ['No event 0 (-)', 'Event 1 (+)', 'Event 2 (+)','Event 3 (+)']\n",
    "print(classification_report(Y_true, Y_pred, target_names=target_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the overal accuracy and the confusion matrix, other important criteria such as precision and recall of our classifier are shown in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
