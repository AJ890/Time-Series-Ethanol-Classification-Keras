{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# cnn model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Ethanol.csv', header = None)\n",
    "n_features=1\n",
    "X = df.drop(df.columns[[0]], axis=1).values\n",
    "y = df[df.columns[[0]]].values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(\n",
    "                                    X, y, test_size=0.10,random_state=42)\n",
    "trainX=trainX.reshape((trainX.shape[0], trainX.shape[1], n_features))\n",
    "testX=testX.reshape((testX.shape[0], testX.shape[1], n_features))\n",
    "trainy = trainy - 1\n",
    "testy = testy - 1\n",
    "# one hot encode y\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_timesteps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4a9e20acbca4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mn_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'n_timesteps' is not defined"
     ]
    }
   ],
   "source": [
    "n_timesteps,n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "903/903 [==============================] - 2s 2ms/step - loss: 1.4239 - acc: 0.2746\n",
      "Epoch 2/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 1.3620 - acc: 0.3389\n",
      "Epoch 3/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 1.3351 - acc: 0.3965\n",
      "Epoch 4/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 1.2726 - acc: 0.4374\n",
      "Epoch 5/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 1.2649 - acc: 0.3865\n",
      "Epoch 6/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 1.1852 - acc: 0.4928\n",
      "Epoch 7/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 1.1305 - acc: 0.5249\n",
      "Epoch 8/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 1.0782 - acc: 0.5271\n",
      "Epoch 9/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 1.0302 - acc: 0.5670\n",
      "Epoch 10/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.9563 - acc: 0.6024\n",
      "Epoch 11/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.9320 - acc: 0.5991\n",
      "Epoch 12/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.9170 - acc: 0.5980\n",
      "Epoch 13/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.9076 - acc: 0.6113\n",
      "Epoch 14/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.8182 - acc: 0.6589\n",
      "Epoch 15/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.7992 - acc: 0.6800\n",
      "Epoch 16/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.7751 - acc: 0.6733\n",
      "Epoch 17/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.7250 - acc: 0.7076\n",
      "Epoch 18/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.6886 - acc: 0.7287A: 0s - loss: 0.6896 - ac\n",
      "Epoch 19/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.6766 - acc: 0.7409\n",
      "Epoch 20/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.6602 - acc: 0.7486\n",
      "Epoch 21/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.6147 - acc: 0.7752\n",
      "Epoch 22/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.5962 - acc: 0.7586\n",
      "Epoch 23/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.5911 - acc: 0.7630\n",
      "Epoch 24/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.5658 - acc: 0.7774\n",
      "Epoch 25/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.5349 - acc: 0.8051\n",
      "Epoch 26/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.5346 - acc: 0.7885\n",
      "Epoch 27/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.5103 - acc: 0.7973\n",
      "Epoch 28/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4801 - acc: 0.8361\n",
      "Epoch 29/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4527 - acc: 0.8350\n",
      "Epoch 30/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4523 - acc: 0.8261\n",
      "Epoch 31/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4634 - acc: 0.8073\n",
      "Epoch 32/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4307 - acc: 0.8483\n",
      "Epoch 33/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4485 - acc: 0.8361\n",
      "Epoch 34/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4031 - acc: 0.8461\n",
      "Epoch 35/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4137 - acc: 0.8416\n",
      "Epoch 36/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3903 - acc: 0.8660\n",
      "Epoch 37/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3719 - acc: 0.8660\n",
      "Epoch 38/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3699 - acc: 0.8738\n",
      "Epoch 39/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3722 - acc: 0.8560\n",
      "Epoch 40/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3660 - acc: 0.8571\n",
      "Epoch 41/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4088 - acc: 0.8261\n",
      "Epoch 42/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.4224 - acc: 0.8140\n",
      "Epoch 43/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3399 - acc: 0.8848\n",
      "Epoch 44/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3403 - acc: 0.8627\n",
      "Epoch 45/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3934 - acc: 0.8317\n",
      "Epoch 46/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3472 - acc: 0.8771\n",
      "Epoch 47/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3287 - acc: 0.8749\n",
      "Epoch 48/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2925 - acc: 0.9025\n",
      "Epoch 49/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2948 - acc: 0.8926\n",
      "Epoch 50/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3276 - acc: 0.8749\n",
      "Epoch 51/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2871 - acc: 0.8992\n",
      "Epoch 52/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2786 - acc: 0.9103\n",
      "Epoch 53/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2671 - acc: 0.9059\n",
      "Epoch 54/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2693 - acc: 0.9037\n",
      "Epoch 55/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2957 - acc: 0.8893\n",
      "Epoch 56/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2688 - acc: 0.9025\n",
      "Epoch 57/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2570 - acc: 0.9014\n",
      "Epoch 58/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2597 - acc: 0.9014\n",
      "Epoch 59/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2410 - acc: 0.9070\n",
      "Epoch 60/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2580 - acc: 0.8992\n",
      "Epoch 61/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2666 - acc: 0.9048\n",
      "Epoch 62/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2466 - acc: 0.9092\n",
      "Epoch 63/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2330 - acc: 0.9169A: 0s - loss: 0.2379 - acc: 0.\n",
      "Epoch 64/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2379 - acc: 0.9092\n",
      "Epoch 65/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2697 - acc: 0.8893\n",
      "Epoch 66/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2232 - acc: 0.9324\n",
      "Epoch 67/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2179 - acc: 0.9347\n",
      "Epoch 68/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2165 - acc: 0.9214\n",
      "Epoch 69/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2050 - acc: 0.9291\n",
      "Epoch 70/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2598 - acc: 0.8948\n",
      "Epoch 71/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2202 - acc: 0.9147\n",
      "Epoch 72/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1951 - acc: 0.9280\n",
      "Epoch 73/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1903 - acc: 0.9435\n",
      "Epoch 74/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1901 - acc: 0.9302\n",
      "Epoch 75/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1743 - acc: 0.9402\n",
      "Epoch 76/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1835 - acc: 0.9347\n",
      "Epoch 77/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1641 - acc: 0.9457\n",
      "Epoch 78/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1724 - acc: 0.9446\n",
      "Epoch 79/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1718 - acc: 0.9413A: 0s - loss: 0.1591 -\n",
      "Epoch 80/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1791 - acc: 0.9402\n",
      "Epoch 81/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2217 - acc: 0.9203\n",
      "Epoch 82/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1774 - acc: 0.9380\n",
      "Epoch 83/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1833 - acc: 0.9347\n",
      "Epoch 84/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1637 - acc: 0.9402\n",
      "Epoch 85/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1641 - acc: 0.9502\n",
      "Epoch 86/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1834 - acc: 0.9380\n",
      "Epoch 87/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2085 - acc: 0.9258\n",
      "Epoch 88/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1759 - acc: 0.9413\n",
      "Epoch 89/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1316 - acc: 0.9579\n",
      "Epoch 90/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1463 - acc: 0.9557\n",
      "Epoch 91/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1324 - acc: 0.9524\n",
      "Epoch 92/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1541 - acc: 0.9502\n",
      "Epoch 93/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1688 - acc: 0.9380\n",
      "Epoch 94/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1655 - acc: 0.9435A: 0s - loss: 0.136\n",
      "Epoch 95/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.3271 - acc: 0.8782\n",
      "Epoch 96/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2057 - acc: 0.9147\n",
      "Epoch 97/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1627 - acc: 0.9369\n",
      "Epoch 98/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1733 - acc: 0.9424\n",
      "Epoch 99/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1463 - acc: 0.9446\n",
      "Epoch 100/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2746 - acc: 0.8926\n",
      "Epoch 101/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1551 - acc: 0.9502\n",
      "Epoch 102/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1282 - acc: 0.9657\n",
      "Epoch 103/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1291 - acc: 0.9612\n",
      "Epoch 104/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1162 - acc: 0.9635\n",
      "Epoch 105/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2093 - acc: 0.9203\n",
      "Epoch 106/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1540 - acc: 0.9424\n",
      "Epoch 107/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1170 - acc: 0.9612\n",
      "Epoch 108/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1637 - acc: 0.9313\n",
      "Epoch 109/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1161 - acc: 0.9679\n",
      "Epoch 110/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1027 - acc: 0.9701\n",
      "Epoch 111/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1049 - acc: 0.9657\n",
      "Epoch 112/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1094 - acc: 0.9646\n",
      "Epoch 113/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0978 - acc: 0.9657\n",
      "Epoch 114/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1058 - acc: 0.9668\n",
      "Epoch 115/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1025 - acc: 0.9657\n",
      "Epoch 116/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1249 - acc: 0.9524\n",
      "Epoch 117/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1346 - acc: 0.9513\n",
      "Epoch 118/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1177 - acc: 0.9601\n",
      "Epoch 119/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0949 - acc: 0.9756\n",
      "Epoch 120/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1833 - acc: 0.9214\n",
      "Epoch 121/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1126 - acc: 0.9623\n",
      "Epoch 122/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1147 - acc: 0.9568\n",
      "Epoch 123/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0914 - acc: 0.9756\n",
      "Epoch 124/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0840 - acc: 0.9790\n",
      "Epoch 125/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0849 - acc: 0.9756\n",
      "Epoch 126/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0864 - acc: 0.9701\n",
      "Epoch 127/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0841 - acc: 0.9779\n",
      "Epoch 128/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0742 - acc: 0.9790\n",
      "Epoch 129/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0788 - acc: 0.9790\n",
      "Epoch 130/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0891 - acc: 0.9679\n",
      "Epoch 131/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1020 - acc: 0.9701\n",
      "Epoch 132/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0980 - acc: 0.9679\n",
      "Epoch 133/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0776 - acc: 0.9779\n",
      "Epoch 134/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0986 - acc: 0.9679\n",
      "Epoch 135/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0911 - acc: 0.9668\n",
      "Epoch 136/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0814 - acc: 0.9756\n",
      "Epoch 137/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1679 - acc: 0.9391\n",
      "Epoch 138/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0938 - acc: 0.9701\n",
      "Epoch 139/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0755 - acc: 0.9756\n",
      "Epoch 140/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0733 - acc: 0.9779\n",
      "Epoch 141/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1378 - acc: 0.9535\n",
      "Epoch 142/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0959 - acc: 0.9712\n",
      "Epoch 143/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0740 - acc: 0.9856\n",
      "Epoch 144/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0558 - acc: 0.9845\n",
      "Epoch 145/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0603 - acc: 0.9834\n",
      "Epoch 146/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0810 - acc: 0.9745\n",
      "Epoch 147/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0611 - acc: 0.9812\n",
      "Epoch 148/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0916 - acc: 0.9646\n",
      "Epoch 149/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0763 - acc: 0.9790\n",
      "Epoch 150/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0569 - acc: 0.9889\n",
      "Train on 903 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0600 - acc: 0.9823 - val_loss: 0.3668 - val_acc: 0.8614\n",
      "Epoch 2/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0682 - acc: 0.9779 - val_loss: 0.3396 - val_acc: 0.8614\n",
      "Epoch 3/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0719 - acc: 0.9767 - val_loss: 0.3512 - val_acc: 0.8713\n",
      "Epoch 4/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0500 - acc: 0.9889 - val_loss: 0.3478 - val_acc: 0.8416\n",
      "Epoch 5/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0552 - acc: 0.9878 - val_loss: 0.3390 - val_acc: 0.8614\n",
      "Epoch 6/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0554 - acc: 0.9801 - val_loss: 0.3157 - val_acc: 0.8812\n",
      "Epoch 7/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0574 - acc: 0.9812 - val_loss: 0.4481 - val_acc: 0.8317\n",
      "Epoch 8/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1219 - acc: 0.9590 - val_loss: 0.3665 - val_acc: 0.8614\n",
      "Epoch 9/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0578 - acc: 0.9867 - val_loss: 0.3360 - val_acc: 0.8713\n",
      "Epoch 10/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0568 - acc: 0.9856 - val_loss: 0.3448 - val_acc: 0.8614\n",
      "Epoch 11/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0524 - acc: 0.9867 - val_loss: 0.3833 - val_acc: 0.8317\n",
      "Epoch 12/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0735 - acc: 0.9734 - val_loss: 0.3218 - val_acc: 0.8614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0603 - acc: 0.9845 - val_loss: 0.3607 - val_acc: 0.8416\n",
      "Epoch 14/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0803 - acc: 0.9745 - val_loss: 0.3221 - val_acc: 0.8812\n",
      "Epoch 15/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0577 - acc: 0.9867 - val_loss: 0.4010 - val_acc: 0.8713\n",
      "Epoch 16/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0585 - acc: 0.9867 - val_loss: 0.3698 - val_acc: 0.8416\n",
      "Epoch 17/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0557 - acc: 0.9867 - val_loss: 0.4023 - val_acc: 0.8515\n",
      "Epoch 18/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0461 - acc: 0.9889 - val_loss: 0.3540 - val_acc: 0.8515\n",
      "Epoch 19/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0435 - acc: 0.9934 - val_loss: 0.4176 - val_acc: 0.8614\n",
      "Epoch 20/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0819 - acc: 0.9767 - val_loss: 0.3277 - val_acc: 0.8614\n",
      "Epoch 21/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0897 - acc: 0.9712 - val_loss: 0.3685 - val_acc: 0.8416\n",
      "Epoch 22/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0594 - acc: 0.9834 - val_loss: 0.3729 - val_acc: 0.8317\n",
      "Epoch 23/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0566 - acc: 0.9812 - val_loss: 0.3306 - val_acc: 0.8812\n",
      "Epoch 24/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0393 - acc: 0.9934 - val_loss: 0.3387 - val_acc: 0.8416\n",
      "Epoch 25/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0470 - acc: 0.9911 - val_loss: 0.3773 - val_acc: 0.8515\n",
      "Epoch 26/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0468 - acc: 0.9845 - val_loss: 0.3299 - val_acc: 0.8614\n",
      "Epoch 27/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0484 - acc: 0.9856 - val_loss: 0.3552 - val_acc: 0.8416\n",
      "Epoch 28/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0575 - acc: 0.9823 - val_loss: 0.3306 - val_acc: 0.8614\n",
      "Epoch 29/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0485 - acc: 0.9834 - val_loss: 0.3782 - val_acc: 0.8515\n",
      "Epoch 30/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0403 - acc: 0.9945 - val_loss: 0.3271 - val_acc: 0.8713\n",
      "Epoch 31/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0414 - acc: 0.9934 - val_loss: 0.3359 - val_acc: 0.8614\n",
      "Epoch 32/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0431 - acc: 0.9911 - val_loss: 0.3314 - val_acc: 0.8515\n",
      "Epoch 33/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0388 - acc: 0.9911 - val_loss: 0.3462 - val_acc: 0.8515\n",
      "Epoch 34/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0381 - acc: 0.9889 - val_loss: 0.3829 - val_acc: 0.8317\n",
      "Epoch 35/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0380 - acc: 0.9934 - val_loss: 0.4133 - val_acc: 0.8317\n",
      "Epoch 36/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0473 - acc: 0.9900 - val_loss: 0.3557 - val_acc: 0.8515\n",
      "Epoch 37/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0326 - acc: 0.9967 - val_loss: 0.3561 - val_acc: 0.8515\n",
      "Epoch 38/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0299 - acc: 0.9956 - val_loss: 0.3377 - val_acc: 0.8911\n",
      "Epoch 39/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0397 - acc: 0.9889 - val_loss: 0.3506 - val_acc: 0.8614\n",
      "Epoch 40/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0430 - acc: 0.9900 - val_loss: 0.3515 - val_acc: 0.8614\n",
      "Epoch 41/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0471 - acc: 0.9900 - val_loss: 0.3600 - val_acc: 0.8317\n",
      "Epoch 42/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0386 - acc: 0.9922 - val_loss: 0.3456 - val_acc: 0.8416\n",
      "Epoch 43/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0402 - acc: 0.9900 - val_loss: 0.3170 - val_acc: 0.8812\n",
      "Epoch 44/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0561 - acc: 0.9756 - val_loss: 0.3618 - val_acc: 0.8713\n",
      "Epoch 45/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0442 - acc: 0.9878 - val_loss: 0.3395 - val_acc: 0.8713\n",
      "Epoch 46/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0464 - acc: 0.9845 - val_loss: 0.3382 - val_acc: 0.8713\n",
      "Epoch 47/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0433 - acc: 0.9889 - val_loss: 0.3914 - val_acc: 0.8317\n",
      "Epoch 48/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0395 - acc: 0.9911 - val_loss: 0.3881 - val_acc: 0.8713\n",
      "Epoch 49/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0389 - acc: 0.9878 - val_loss: 0.3465 - val_acc: 0.8713\n",
      "Epoch 50/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0412 - acc: 0.9900 - val_loss: 0.3753 - val_acc: 0.8416\n",
      "Epoch 51/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0386 - acc: 0.9911 - val_loss: 0.3554 - val_acc: 0.8713\n",
      "Epoch 52/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0336 - acc: 0.9934 - val_loss: 0.4249 - val_acc: 0.8515\n",
      "Epoch 53/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0422 - acc: 0.9889 - val_loss: 0.3658 - val_acc: 0.8713\n",
      "Epoch 54/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0397 - acc: 0.9934 - val_loss: 0.3822 - val_acc: 0.8614\n",
      "Epoch 55/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0385 - acc: 0.9900 - val_loss: 0.3566 - val_acc: 0.8416\n",
      "Epoch 56/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0506 - acc: 0.9790 - val_loss: 0.3353 - val_acc: 0.8812\n",
      "Epoch 57/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0341 - acc: 0.9967 - val_loss: 0.3685 - val_acc: 0.8515\n",
      "Epoch 58/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0374 - acc: 0.9889 - val_loss: 0.3502 - val_acc: 0.8713\n",
      "Epoch 59/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0297 - acc: 0.9945 - val_loss: 0.3936 - val_acc: 0.8515\n",
      "Epoch 60/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0373 - acc: 0.9878 - val_loss: 0.4276 - val_acc: 0.8416\n",
      "Epoch 61/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0380 - acc: 0.9922 - val_loss: 0.3591 - val_acc: 0.8515\n",
      "Epoch 62/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0282 - acc: 0.9945 - val_loss: 0.3657 - val_acc: 0.8515\n",
      "Epoch 63/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0306 - acc: 0.9911 - val_loss: 0.3872 - val_acc: 0.8416\n",
      "Epoch 64/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0366 - acc: 0.9889 - val_loss: 0.3342 - val_acc: 0.8812\n",
      "Epoch 65/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0344 - acc: 0.9934 - val_loss: 0.4223 - val_acc: 0.8416\n",
      "Epoch 66/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0352 - acc: 0.9889 - val_loss: 0.3542 - val_acc: 0.8515\n",
      "Epoch 67/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0682 - acc: 0.9801 - val_loss: 0.3853 - val_acc: 0.8218\n",
      "Epoch 68/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0702 - acc: 0.9734 - val_loss: 0.3462 - val_acc: 0.8416\n",
      "Epoch 69/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.2088 - acc: 0.9181 - val_loss: 0.4137 - val_acc: 0.8416\n",
      "Epoch 70/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1113 - acc: 0.9568 - val_loss: 0.4131 - val_acc: 0.8515\n",
      "Epoch 71/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0671 - acc: 0.9790 - val_loss: 0.4274 - val_acc: 0.8416\n",
      "Epoch 72/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0461 - acc: 0.9856 - val_loss: 0.3342 - val_acc: 0.8911\n",
      "Epoch 73/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0454 - acc: 0.9867 - val_loss: 0.3215 - val_acc: 0.8911\n",
      "Epoch 74/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0463 - acc: 0.9834 - val_loss: 0.3241 - val_acc: 0.8713\n",
      "Epoch 75/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0456 - acc: 0.9878 - val_loss: 0.3443 - val_acc: 0.8614\n",
      "Epoch 76/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0398 - acc: 0.9878 - val_loss: 0.3484 - val_acc: 0.8218\n",
      "Epoch 77/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0346 - acc: 0.9900 - val_loss: 0.3502 - val_acc: 0.8713\n",
      "Epoch 78/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0353 - acc: 0.9900 - val_loss: 0.3574 - val_acc: 0.8713\n",
      "Epoch 79/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0276 - acc: 0.9945 - val_loss: 0.3882 - val_acc: 0.8317\n",
      "Epoch 80/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0208 - acc: 0.9978 - val_loss: 0.3422 - val_acc: 0.8614\n",
      "Epoch 81/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0371 - acc: 0.9867 - val_loss: 0.3888 - val_acc: 0.8515\n",
      "Epoch 82/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0431 - acc: 0.9834 - val_loss: 0.4008 - val_acc: 0.8515\n",
      "Epoch 83/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0285 - acc: 0.9967 - val_loss: 0.4405 - val_acc: 0.8218\n",
      "Epoch 84/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0868 - acc: 0.9756 - val_loss: 0.4040 - val_acc: 0.8317\n",
      "Epoch 85/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0406 - acc: 0.9867 - val_loss: 0.3748 - val_acc: 0.8416\n",
      "Epoch 86/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0399 - acc: 0.9856 - val_loss: 0.4138 - val_acc: 0.8317\n",
      "Epoch 87/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0323 - acc: 0.9900 - val_loss: 0.3917 - val_acc: 0.8614\n",
      "Epoch 88/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0344 - acc: 0.9889 - val_loss: 0.3878 - val_acc: 0.8713\n",
      "Epoch 89/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0417 - acc: 0.9867 - val_loss: 0.4637 - val_acc: 0.8515\n",
      "Epoch 90/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0354 - acc: 0.9900 - val_loss: 0.3786 - val_acc: 0.8614\n",
      "Epoch 91/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0372 - acc: 0.9856 - val_loss: 0.4442 - val_acc: 0.8218\n",
      "Epoch 92/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1377 - acc: 0.9457 - val_loss: 0.4250 - val_acc: 0.8218\n",
      "Epoch 93/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1413 - acc: 0.9480 - val_loss: 0.3701 - val_acc: 0.8515\n",
      "Epoch 94/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.1379 - acc: 0.9612 - val_loss: 0.4013 - val_acc: 0.8713\n",
      "Epoch 95/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0662 - acc: 0.9779 - val_loss: 0.3447 - val_acc: 0.9010\n",
      "Epoch 96/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0472 - acc: 0.9889 - val_loss: 0.3433 - val_acc: 0.8713\n",
      "Epoch 97/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0373 - acc: 0.9922 - val_loss: 0.3419 - val_acc: 0.8713\n",
      "Epoch 98/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0409 - acc: 0.9889 - val_loss: 0.4124 - val_acc: 0.8713\n",
      "Epoch 99/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0559 - acc: 0.9823 - val_loss: 0.3978 - val_acc: 0.8911\n",
      "Epoch 100/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0511 - acc: 0.9790 - val_loss: 0.4333 - val_acc: 0.8218\n",
      "Epoch 101/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0341 - acc: 0.9934 - val_loss: 0.4122 - val_acc: 0.8317\n",
      "Epoch 102/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0371 - acc: 0.9889 - val_loss: 0.4301 - val_acc: 0.8218\n",
      "Epoch 103/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0304 - acc: 0.9945 - val_loss: 0.4141 - val_acc: 0.8812\n",
      "Epoch 104/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0274 - acc: 0.9945 - val_loss: 0.3969 - val_acc: 0.8713\n",
      "Epoch 105/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0291 - acc: 0.9934 - val_loss: 0.4098 - val_acc: 0.8317\n",
      "Epoch 106/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0363 - acc: 0.9889 - val_loss: 0.3843 - val_acc: 0.8812\n",
      "Epoch 107/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0239 - acc: 0.9956 - val_loss: 0.3925 - val_acc: 0.8218\n",
      "Epoch 108/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0237 - acc: 0.9978 - val_loss: 0.4002 - val_acc: 0.8812\n",
      "Epoch 109/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0328 - acc: 0.9889 - val_loss: 0.3955 - val_acc: 0.8317\n",
      "Epoch 110/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0283 - acc: 0.9934 - val_loss: 0.4048 - val_acc: 0.8020\n",
      "Epoch 111/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0248 - acc: 0.9945 - val_loss: 0.4164 - val_acc: 0.8416\n",
      "Epoch 112/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0770 - acc: 0.9734 - val_loss: 0.4115 - val_acc: 0.8614\n",
      "Epoch 113/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0533 - acc: 0.9834 - val_loss: 0.4214 - val_acc: 0.8020\n",
      "Epoch 114/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0477 - acc: 0.9845 - val_loss: 0.3802 - val_acc: 0.8218\n",
      "Epoch 115/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0476 - acc: 0.9845 - val_loss: 0.4449 - val_acc: 0.8317\n",
      "Epoch 116/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0245 - acc: 0.9934 - val_loss: 0.3814 - val_acc: 0.8713\n",
      "Epoch 117/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0238 - acc: 0.9945 - val_loss: 0.3860 - val_acc: 0.8614\n",
      "Epoch 118/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0395 - acc: 0.9856 - val_loss: 0.4123 - val_acc: 0.8713\n",
      "Epoch 119/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0224 - acc: 0.9967 - val_loss: 0.4044 - val_acc: 0.8713\n",
      "Epoch 120/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0193 - acc: 0.9967 - val_loss: 0.3520 - val_acc: 0.8812\n",
      "Epoch 121/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0182 - acc: 0.9956 - val_loss: 0.3982 - val_acc: 0.8812\n",
      "Epoch 122/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0184 - acc: 0.9989 - val_loss: 0.3538 - val_acc: 0.8614\n",
      "Epoch 123/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0161 - acc: 0.9978 - val_loss: 0.4149 - val_acc: 0.8812\n",
      "Epoch 124/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0199 - acc: 0.9967 - val_loss: 0.3566 - val_acc: 0.8614\n",
      "Epoch 125/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0218 - acc: 0.9956 - val_loss: 0.3636 - val_acc: 0.8713\n",
      "Epoch 126/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0186 - acc: 0.9956 - val_loss: 0.4314 - val_acc: 0.8614\n",
      "Epoch 127/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0263 - acc: 0.9900 - val_loss: 0.4082 - val_acc: 0.8416\n",
      "Epoch 128/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0240 - acc: 0.9978 - val_loss: 0.4041 - val_acc: 0.8812\n",
      "Epoch 129/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0264 - acc: 0.9922 - val_loss: 0.4275 - val_acc: 0.8713\n",
      "Epoch 130/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0296 - acc: 0.9911 - val_loss: 0.4728 - val_acc: 0.8416\n",
      "Epoch 131/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0322 - acc: 0.9922 - val_loss: 0.3546 - val_acc: 0.8713\n",
      "Epoch 132/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0232 - acc: 0.9945 - val_loss: 0.3482 - val_acc: 0.8812\n",
      "Epoch 133/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0185 - acc: 0.9945 - val_loss: 0.3986 - val_acc: 0.8614\n",
      "Epoch 134/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0253 - acc: 0.9911 - val_loss: 0.3488 - val_acc: 0.8515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0390 - acc: 0.9845 - val_loss: 0.3188 - val_acc: 0.8713\n",
      "Epoch 136/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0255 - acc: 0.9922 - val_loss: 0.3997 - val_acc: 0.8218\n",
      "Epoch 137/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0258 - acc: 0.9911 - val_loss: 0.4217 - val_acc: 0.8713\n",
      "Epoch 138/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0263 - acc: 0.9922 - val_loss: 0.4174 - val_acc: 0.8614\n",
      "Epoch 139/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0284 - acc: 0.9889 - val_loss: 0.4069 - val_acc: 0.8416\n",
      "Epoch 140/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0198 - acc: 0.9978 - val_loss: 0.4319 - val_acc: 0.8317\n",
      "Epoch 141/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0205 - acc: 0.9934 - val_loss: 0.4082 - val_acc: 0.8614\n",
      "Epoch 142/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0625 - acc: 0.9823 - val_loss: 0.4366 - val_acc: 0.8317\n",
      "Epoch 143/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0310 - acc: 0.9900 - val_loss: 0.3850 - val_acc: 0.8416\n",
      "Epoch 144/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0326 - acc: 0.9867 - val_loss: 0.4280 - val_acc: 0.8317\n",
      "Epoch 145/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0285 - acc: 0.9911 - val_loss: 0.3667 - val_acc: 0.8416\n",
      "Epoch 146/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0251 - acc: 0.9934 - val_loss: 0.4486 - val_acc: 0.8218\n",
      "Epoch 147/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0213 - acc: 0.9967 - val_loss: 0.3883 - val_acc: 0.8713\n",
      "Epoch 148/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0180 - acc: 0.9945 - val_loss: 0.4057 - val_acc: 0.8515\n",
      "Epoch 149/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0158 - acc: 0.9978 - val_loss: 0.4317 - val_acc: 0.8416\n",
      "Epoch 150/150\n",
      "903/903 [==============================] - 1s 1ms/step - loss: 0.0333 - acc: 0.9889 - val_loss: 0.4008 - val_acc: 0.8614\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 1732, 15)          315       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1732, 15)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 866, 15)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 12990)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               1662848   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,663,679\n",
      "Trainable params: 1,663,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# fit and evaluate a model\n",
    "\n",
    "verbose, epochs, batch_size = 1, 150, 50\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=15, kernel_size=20, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "#model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "#opt = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "checkpoint = ModelCheckpoint(\"MODEL\", monitor='val_loss', verbose=0, \n",
    "                                 save_best_only=True, mode='min')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "Trained=model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=1 ,shuffle=True, \n",
    "                      callbacks=[checkpoint], validation_data=(testX, testy))\n",
    "\n",
    "# evaluate model\n",
    "\n",
    "\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Loss')\n",
    "pyplot.plot(Trained.history['loss'],     label='train')\n",
    "#pyplot.plot(Trained.history['val_loss'], label='eval')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[16  2  0  0]\n",
      " [ 3 15  5  0]\n",
      " [ 0  2 27  0]\n",
      " [ 0  0  0 31]]\n",
      "\n",
      " Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No event 0 (-)       0.84      0.89      0.86        18\n",
      "   Event 1 (+)       0.79      0.65      0.71        23\n",
      "   Event 2 (+)       0.84      0.93      0.89        29\n",
      "   Event 3 (+)       1.00      1.00      1.00        31\n",
      "\n",
      "     micro avg       0.88      0.88      0.88       101\n",
      "     macro avg       0.87      0.87      0.87       101\n",
      "  weighted avg       0.88      0.88      0.88       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model1 = tf.keras.models.load_model('MODEL')\n",
    "\n",
    "a = model1.predict(testX)\n",
    "b = np.zeros_like(a)\n",
    "b[np.arange(len(a)), a.argmax(1)] = 1\n",
    "\n",
    "\n",
    "Y_pred = np.argmax(b, axis=1)\n",
    "Y_true = np.argmax(testy,  axis=1)\n",
    "\n",
    "#for i in range(150,160):\n",
    "#    print(Y_pred[i], Y_true[i])\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_true, Y_pred))\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "target_classes = ['No event 0 (-)', 'Event 1 (+)', 'Event 2 (+)','Event 3 (+)']\n",
    "print(classification_report(Y_true, Y_pred, target_names=target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=logs/ \n",
    "#first_layer=75\n",
    "dense_layers = [64,256]\n",
    "filters = [15,40,28]\n",
    "kernels = [15,20,24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-kernel-40-filters-128-node-1554105633\n",
      "30-kernel-40-filters-128-node-1554106104\n",
      "20-kernel-15-filters-128-node-1554106578\n",
      "30-kernel-15-filters-128-node-1554106790\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dense_nodes in dense_layers:#loops sobre dense layers\n",
    "    for filter1 in filters:#loops sobre nodos\n",
    "        for kernel in kernels:\n",
    "            NAME = \"{}-kernel-{}-filters-{}-node-{}\".format(kernel, filter1, dense_nodes, int(time.time()))#Nombrar cada iteracion\n",
    "            print(NAME)\n",
    "            \n",
    "            n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "            model = Sequential()\n",
    "            model.add(Conv1D(filters=filter1, kernel_size=kernel, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(Flatten())\n",
    "\n",
    "            #Estas son las capas ocultas que se pueden modificar\n",
    "            #for _ in range(dense_layer):\n",
    "               # model.add(Dense(64, activation='relu'))       \n",
    "               # model.add(Dropout(0.2))\n",
    "                     \n",
    "            \n",
    "            model.add(Dense(dense_nodes, activation='relu'))\n",
    "            model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "            #Verifcacion en Tensorboard\n",
    "            \n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "            filepath=\"mod/{}\".format(NAME)\n",
    "            \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, \n",
    "                                 save_best_only=True, mode='min')\n",
    "            #sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "            #ada=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "            \n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=0 ,shuffle=True, \n",
    "                      callbacks=[tensorboard,checkpoint], validation_data=(testX, testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
